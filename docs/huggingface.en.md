# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP. At present, 137 models and 27 datasets are listed.

[English](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.en.md) | [日本語 (Japanese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.ja.md) | [繁體中文 (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hant.md) | [简体中文 (Chinese) ](https://github.com/taishi-i/awesome-japanese-nlp-resources/blob/main/docs/huggingface.zh-hans.md)


## Contents

 * [Models](#models)
 * [Datasets](#datasets)


## Models

 * [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - xlm-roberta-ner-japanese (Japanese caption : 日本語の固有表現抽出のモデル)This model is a fine-tuned version of xlm-roberta-base (pre-trained cross-lingual RobertaModel) trained for named entity recognition (NER) token classification.
 * [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - Fine-tuned XLSR-53 large model for speech recognition in Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * [cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking) - BERT base Japanese (IPA dictionary, whole word masking enabled)
 * [cl-tohoku/bert-base-japanese](https://huggingface.co/cl-tohoku/bert-base-japanese) - BERT base Japanese (IPA dictionary)
 * [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - bert-finetuned-japanese-sentiment This model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset.
 * [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
 * [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - This is a Japanese sentence-BERT model.
 * [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * [cl-tohoku/bert-base-japanese-char](https://huggingface.co/cl-tohoku/bert-base-japanese-char) - BERT base Japanese (character tokenization)
 * [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja) - Sentence BERT base Japanese model This repository contains a Sentence BERT base model for Japanese.
 * [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
 * [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)
 * [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - OpenCALM-7B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en) - FuguMT
 * [cl-tohoku/bert-base-japanese-v3](https://huggingface.co/cl-tohoku/bert-base-japanese-v3) - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)
 * [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - rinna/japanese-club-vit-b-16
 * [cl-tohoku/bert-base-japanese-char-v2](https://huggingface.co/cl-tohoku/bert-base-japanese-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)
 * [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation) - Model for generating titles from article text SEE:
 * [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - japanese-roberta-base This repository provides a base-sized Japanese RoBERTa model.
 * [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ELYZA-japanese-Llama-2-7b Model Description
ELYZA-japanese-Llama-2-7b is a model that has undergone additional pre-training to enhance its Japanese language capabilities based on Llama2.
 * [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony) - BERT Base Japanese for Irony
 * [sociocom/MedNER-CR-JA](https://huggingface.co/sociocom/MedNER-CR-JA) - This is a model for named entity recognition of Japanese medical documents.
 * [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - japanese-sentiment-analysis This model was trained from scratch on the chABSA dataset.
 * [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - 日本語T5事前学習済みモデル This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
 * [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - This is a Japanese sentence-LUKE model.
 * [staka/fugumt-en-ja](https://huggingface.co/staka/fugumt-en-ja) - FuguMT
 * [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - Japanese SimCSE (BERT-base) 日本語のREADME/Japanese README summary model name: pkshatech/simcse-ja-bert-base-clcmlp This is a Japanese SimCSE model.
 * [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - This model is a fine-tuned version of Luke-japanese-large-lite.
 * [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 tiny Model description This is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b) - japanese-gpt-neox-3.6b Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ELYZA-japanese-Llama-2-7b Model Description
ELYZA-japanese-Llama-2-7b is a model that has undergone additional pre-training to enhance its Japanese language capabilities based on Llama2.
 * [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - gpt-neox-japanese-2.7b
 * [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - LINE DistilBERT Japanese This is a DistilBERT model pre-trained on 131 GB of Japanese web text.
 * [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft) - japanese-large-lm-3.6b-instruction-sft
 * [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b) - japanese-gpt-1b This repository provides a 1.3B-parameter Japanese GPT model.
 * [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha) - Japanese InstructBLIP Alpha Model Details Japanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions.
 * [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) - OpenCALM-3B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - OpenCALM-Large Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - OpenCALM-1B Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - OpenCALM-Small Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - Model Card for Japanese DeBERTa V2 tiny Model description

Model Name: Japanese DeBERTa V2 tiny
Model Version: 2.0
Model Size: 50MB

Model Description:
The Japanese DeBERTa V2 tiny model is a language model specifically trained for Japanese text. It is a smaller version of the original DeBERTa model, designed to be more lightweight and efficient while still maintaining high performance. This model is capable of understanding and generating Japanese text, making it suitable for various natural language processing tasks such as text classification, sentiment analysis, and machine translation. It has been trained on a large corpus of Japanese text data and fine-tuned using various language modeling objectives. The model exhibits strong language understanding capabilities and can generate coherent and contextually relevant Japanese text. It is particularly useful for applications that require Japanese language processing.
 * [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b) - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
 * [cl-tohoku/bert-large-japanese-v2](https://huggingface.co/cl-tohoku/bert-large-japanese-v2) - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)
 * [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ELYZA-japanese-Llama-2-7b Model Description
ELYZA-japanese-Llama-2-7b is a model that has undergone additional pre-training to enhance Japanese language capabilities based on Llama2.
 * [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2) - Japanese GPT-NeoX 3.6b Instruction SFT V2 Overview
 * [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base) - japanese-hubert-base This is a Japanese HuBERT (Hidden Unit Bidirectional Encoder Representations from Transformers) model trained by rinna Co., Ltd.
 * [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium) - OpenCALM-Medium Model Description OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite) - luke-japanese luke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ELYZA-japanese-Llama-2-7b Model Description
ELYZA-japanese-Llama-2-7b is a model that has undergone additional pre-training to enhance its Japanese language capabilities based on Llama2.
 * [cl-tohoku/bert-large-japanese](https://huggingface.co/cl-tohoku/bert-large-japanese) - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)
 * [speechbrain/lang-id-voxlingua107-ecapa](https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa) - VoxLingua107 ECAPA-TDNN Spoken Language Identification Model Model description This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain.
 * [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - japanese-gpt2-small This repository provides a small-sized Japanese GPT-2 model.
 * [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo) - bilingual-gpt-neox-4b-instruction-ppo Overview This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
 * [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying) - electra-base-cyberbullying This is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
 * [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm) - ku-nlp/roberta-base-japanese-char-wwm Model description This is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking.
 * [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft) - japanese-large-lm-1.7b-instruction-sft This repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
 * [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k) - bilingual-gpt-neox-4b-8k Overview Notice: This model requires transformers&gt;=4.31.0 to work properly.
 * [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - 日本語T5事前学習済みモデル This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
 * [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - nlp-waseda/roberta-large-japanese-seq512 Model description This is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.
 * [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large) - luke-japanese-large luke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * [cl-tohoku/bert-base-japanese-char-v3](https://huggingface.co/cl-tohoku/bert-base-japanese-char-v3) - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)
 * [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b) - stockmark/gpt-neox-japanese-1.4b This repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens.
 * [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli) - This is the model for natural language inference introduced in Chapter 5 of "Introduction to Large-scale Language Models."
 * [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese) - gpt2-large-japanese This repository provides a large sized Japanese GPT-2 model.
 * [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion) - One more step before getting this model.
 * [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) - nlp-waseda/roberta-base-japanese Model description This is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.
 * [cl-tohoku/bert-base-japanese-char-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-char-whole-word-masking) - BERT base Japanese (character tokenization, whole word masking enabled)
 * [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0) - Heron BLIP Japanese StableLM
 * [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja) - The stanza model for Japanese (ja) is a language model specifically designed to generate text in the Japanese language. It is trained on a large corpus of Japanese text and is capable of producing coherent and grammatically correct sentences in Japanese. The model takes into account the unique characteristics of the Japanese language, such as the use of kanji, hiragana, and katakana characters, as well as the complex grammar and sentence structure. With the stanza model for Japanese, users can generate high-quality Japanese text for various applications, including machine translation, text generation, and natural language processing tasks.
 * [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - This is the model introduced in Chapter 5 of "Introduction to Large-scale Language Models" (meaning similarity calculation).
 * [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli) - bert-base-japanese-jsnli This model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset.
 * [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1) - Japanese version CLIP model
 * [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja) - This is the model for (sentiment analysis) introduced in Chapter 5 of "Introduction to Large-scale Language Models".
 * [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b) - This is the model used in AI BunCho.
 * [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-with-auto-jumanpp Model description
 * [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) - This is the model of unsupervised SimCSE introduced in Chapter 8 of "Introduction to Large-scale Language Models."
 * [cl-tohoku/bert-large-japanese-char-v2](https://huggingface.co/cl-tohoku/bert-large-japanese-char-v2) - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)
 * [larryvrh/mt5-translation-ja_zh](https://huggingface.co/larryvrh/mt5-translation-ja_zh) - This is the finetuned version of google/mt5-large for translating Japanese into Simplified Chinese.
 * [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base) - luke-japanese luke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
 * [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 large Model description This is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * [nlp-waseda/roberta-base-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-base-japanese-with-auto-jumanpp) - nlp-waseda/roberta-base-japanese-with-auto-jumanpp Model description
 * [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese) - Model Card for Japanese BART large Model description
 * [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - This is the named entity recognition model introduced in Chapter 6 of the book "Introduction to Large Language Models".
 * [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese) - mt5_summarize_japanese (Japanese caption : 日本語の要約のモデル)This model is a fine-tuned version of google/mt5-small trained for Japanese summarization.
 * [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news) - bart-base-japanese-news(base-sized model)
 * [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering) - RoBERTa base Japanese - JaQuAD Description A Japanese Question Answering model fine-tuned on JaQuAD.Please refer RoBERTa base Japanese for details about the pre-training model.
 * [ku-accms/bert-base-japanese-ssuw](https://huggingface.co/ku-accms/bert-base-japanese-ssuw) - ku-accms/bert-base-japanese-ssuw Model description This is a pre-trained Japanese BERT base model for super short unit words (SSUW).
 * [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000) - alabnii/jmedroberta-base-sentencepiece-vocab50000 Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).
 * [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset) - This is the named entity recognition model introduced in Chapter 6 of the book "Introduction to Large-scale Language Models".
 * [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b) - Update History: On May 7, 2023, we added the "oasst1-89k-ja" dataset and made it compatible with the dialogue system.
 * [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying) - yacis-electra-small-cyberbullying
 * [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese) - COMET-T5 ja Finetuned T5 on ATOMIC ja using a text-to-text language modeling objective.
 * [MuneK/bert-large-japanese-v2-finetuned-wrime](https://huggingface.co/MuneK/bert-large-japanese-v2-finetuned-wrime) - bert-large-japanese-v2-finetuned-wrime
 * [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char) - Model Card for Japanese character-level GPT-2 Medium Model description This is a Japanese character-level GPT-2 Medium (310M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
 * [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0) - Heron GIT Japanese StableLM
 * [izumi-lab/bert-small-japanese-fin](https://huggingface.co/izumi-lab/bert-small-japanese-fin) - BERT small Japanese finance This is a BERT model pretrained on texts in the Japanese language.
 * [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese) - nlp-waseda/roberta-large-japanese Model description This is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.
 * [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - Model Card for Model ID Original model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning.
 * [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite) - luke-japanese-large-lite luke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000) - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000 Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).
 * [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner) - This model is fine-tuned based on luke-japanese-base to be used for named entity recognition (NER).
 * [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese) - nlp-waseda/gpt2-small-japanese This model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.
 * [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese) - XLNet-japanese Model description This model require Mecab and senetencepiece with XLNetTokenizer.
 * [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos) - The bert-base-japanese-upos model is a pre-trained model based on the BERT architecture for Japanese text. It is trained on a large corpus of Japanese text and is specifically designed for part-of-speech tagging tasks.
 * [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese) - 英語+日本語T5事前学習済みモデル This is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus.
 * [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1) - albert-base-japanese-v1 is a pre-trained ALBERT model for Japanese language.
 * [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - alabnii/jmedroberta-base-sentencepiece Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).
 * [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese) - Wav2Vec2-Large-Japanese Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data.
 * [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp) - Model Card for Japanese DeBERTa V2 base Model description This is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.
 * [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base) - This is a model that has made the following changes to studio-ousia/luke-japanese-base.
 * [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional) - Additional pretrained BERT base Japanese finance This is a BERT model pretrained on texts in the Japanese language.
 * [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head) - bert-base-japanese-wikipedia-ud-head Model Description
 * [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr) - Manga OCR Optical character recognition for Japanese text, with the main focus being Japanese manga.
 * [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps) - whisper-large-v2-japanese-5k-steps This model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11)..
 * [sonoisa/t5-base-japanese-article-generation](https://huggingface.co/sonoisa/t5-base-japanese-article-generation) - Model to generate article body from title: SEE
 * [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis) - bert-japanese_finetuned-sentiment-analysis This model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset.
 * [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head) - The model deberta-base-japanese-aozora-ud-head is a Japanese language model trained on the Aozora corpus. It is a base model that includes a head for dependency parsing.
 * [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next) - reazonspeech-espnet-next ReazonSpeech is a project to maintain freely-available Japanese audiodatasets and ML models.reazonspeech-espnet-next is a "bleeding-edge" repository that containslatest ASR models trained by ReazonSpeech team.
 * [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece) - alabnii/jmedroberta-base-manbyo-wordpiece Model description This is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).
 * [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese) - BERT small Japanese finance This is a BERT model pretrained on texts in the Japanese language.
 * [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese) - Model Card for Japanese BART base Model description This is a Japanese BART base model pre-trained on Japanese Wikipedia.
 * [izumi-lab/electra-small-japanese-fin-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-fin-discriminator) - ELECTRA small Japanese finance discriminator This is a ELECTRA model pretrained on texts in the Japanese language.
 * [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - Model Card for Japanese DeBERTa V2 large Model description This is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and theJapanese portion of OSCAR.
 * [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - sbert-jsnli-luke-japanese-base-lite This is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
 * [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall) - I'm sorry, but as an AI language model, I am unable to translate content from Japanese to English. However, you can use online translation tools like Google Translate or DeepL to get an accurate translation.
 * [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) - Model Card for Tanrei/GPTSAN-japanese General-purpose Swich transformer based Japanese language model GPTSAN has some unique features.
 * [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b) - japanese-large-lm-1.7b This repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation.
 * [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - Using the model BertForTokenClassification for Japanese named entity recognition, we extract named entities from Japanese sentences.
 * [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft) - japanese-gpt-neox-3.6b-instruction-sft Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - japanese-gpt-neox-small This repository provides a small-sized Japanese GPT-NeoX model.
 * [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b) - Japanese large language model 3.6 billion parameters
 * [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - japanese-gpt2-medium This repository provides a medium-sized Japanese GPT-2 model.
 * [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - rinna/japanese-clip-vit-b-16
 * [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - This is a Japanese sentence-BERT model.
 * [cl-tohoku/bert-base-japanese-v2](https://huggingface.co/cl-tohoku/bert-base-japanese-v2) - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)
 * [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo) - japanese-gpt-neox-3.6b-instruction-ppo Overview This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b) - Japanese-StableLM-Base-Alpha-7B "A parrot able to speak Japanese, ukiyoe, edo period" — Stable Diffusion XL Model Description japanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance.


## Datasets

 * [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - This is the filtered Japanese subset of XL-Sum followed by PaLM 2filters15-gram overlap* code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86anumber of examplestrain: 4215 (before: 7113)validation: 758 (before: 889)test: 766 (before: 889)
 * [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA) - Dataset Details Dataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset.
 * [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - llm-japanese-dataset Japanese instruction (chat) dataset for LLM construction. It can be used for tuning chat (instruction) response tasks using LoRA, mainly for LLM models constructed in English.
 * [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - Dataset Card for JaQuAD Dataset Summary Japanese Question Answering Dataset (JaQuAD), released in 2022, is ahuman-annotated dataset created for Japanese Machine Reading Comprehension.
 * [datasets/snow_simplified_japanese_corpus](https://huggingface.co/datasets/snow_simplified_japanese_corpus) - Dataset Card for SNOW T15 and T23 (simplified Japanese corpus)
 * [zan/lima-ja](https://huggingface.co/datasets/zan/lima-ja) - Dataset Card for LIMA-JA Dataset Description This is Japanese LIMA dataset, which is translated from the LIMA dataset that Meta's LIMA model (Zhou et al., 2023) was trained on.
 * [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - Dataset Card for "japanese_alpaca_data" This dataset is based on masa3141's great work on japanese-alpaca-lora
 * [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese) - fungi_indexed_mycological_papers_japanese大菌輪「論文3行まとめ」データセット最終更新日：2023/8/26（R3-10914まで）==== Languages Japanese This dataset is available in Japanese only.
 * [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords) - Japanese stopwords for nagisa
 * [hpprc/janli](https://huggingface.co/datasets/hpprc/janli) - Dataset Card for JaNLI Dataset Summary
 * [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - This is a Japanese chat dataset for building a vanilla LLM, extracted from izumi-lab/llm-japanese-dataset, excluding the dataset for Japanese-English translation, etc.
 * [sudy-super/dialogsum-ja](https://huggingface.co/datasets/sudy-super/dialogsum-ja) - This dataset is a Japanese dialogue summary dataset that has been translated from dialogsum and CSDS.
 * [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli) - Dataset Card for JSNLI Dataset Summary: This dataset is a Japanese translation of the SNLI, which is a standard benchmark for natural language inference (NLI), provided by KUROHASHI-CHU-MURAWAKI LAB.
 * [fujiki/guanaco_ja](https://huggingface.co/datasets/fujiki/guanaco_ja) - This is a Japanese portion of the Guanaco dataset.
 * [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja) - range3/wiki40b-ja This dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted.
 * [nakayama/hh-rlhf-helpful-base-ja](https://huggingface.co/datasets/nakayama/hh-rlhf-helpful-base-ja) - This is a translation of the English text listed in the "chosen" section of the "helpful-base" content from the following input: https://github.com/anthropics/hh-rlhf. It has been translated using fuguMT and any translations that were not successful have been excluded and corrected.
 * [larryvrh/WikiMatrix-v1-Ja_Zh-filtered](https://huggingface.co/datasets/larryvrh/WikiMatrix-v1-Ja_Zh-filtered) - Filtered and modified version of Japanese/Chinese language pair data from WikiMatrix v1.Process steps:Basic regex based filtering / length checking to remove abnormal pairs.
 * [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin) - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-cleanfilteredrow["meta"]["文字遣い種別"] == "新字新仮名"
 * [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset) - Sakura_dataset is a commercially available, ultra-small-scale, high-quality Japanese dataset.
 * [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - range3/cc100-ja This dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded.
 * [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ELYZA-tasks-100: Japanese instruction model evaluation dataset Data Description This dataset is an evaluation dataset for models that have undergone instruction tuning.
 * [AhmedSSabir/Japanese-wiki-dump-sentence-dataset](https://huggingface.co/datasets/AhmedSSabir/Japanese-wiki-dump-sentence-dataset) - Dataset 5M (5121625) clean Japanese full sentence with the context.
 * [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k) - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples.
 * [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese) - fungi_diagnostic_chars_comparison_japanese大菌輪「識別形質まとめ」データセット最終更新日：2023/8/26（R3-10914まで）==== Languages Japanese This dataset is available in Japanese only.
 * [datasets/covid_tweets_japanese](https://huggingface.co/datasets/covid_tweets_japanese) - Dataset Card for COVID-19 日本語Twitterデータセット (COVID-19 Japanese Twitter Dataset)
 * [datasets/bsd_ja_en](https://huggingface.co/datasets/bsd_ja_en) - Dataset Card for Business Scene Dialogue
 * [reazon-research/reazonspeech](https://huggingface.co/datasets/reazon-research/reazonspeech) - Dataset Card for ReazonSpeech Dataset Summary ReazonSpeech is a large audio corpus collected from Japanese TV programs.
